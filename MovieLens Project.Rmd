---
title: "MovieLens Project"
author: "Ignacio Gurrola"
date: "5/30/2021"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Introduction
Recommendation Systems are software agents that extract the interests and preferences of individual consumers and make recommendations accordingly.They enables us to offer products or services to new users. Certainly, this is essential for many online businesses such as Netflix, YouTube and Amazon. This presentation will review what a recommendation system is, which type exist and how to code them from scratch in R. 

The document has a structure as follows: Chapter 1 describes the dataset and summarizes the goal of the project and key steps. In chapter 2, It is reviewed the process and techniques used, such as data cleaning, data exploration and visualization, any insights gained, and the modeling approach. Chapter 3 present the modeling results and discuss the model performance. Chapter 4 will conclude with a brief summary of the report, its limitations and future work. At the end of the document you will find the references and bibliography for this work. This project wouldn't be possible without the guidance of Dr. Irizarry during all Data Science sessions. 

In general, code and output are not shown in this PDF presentation. 

## 1.1 Types of recommendation Systems
There are two main types of recommendation systems:

**Content-based recommendation system**- System based on using the features of the books in order to offer similar products. For example, on Amazon Kindle section the system will recommend a regular follower of Dan Brown, other digital books and lectures of the same author or bibliography within the same category.

**Collaborative Recommendation System**- This case does not use the features of the products, but rather the opinions of the users. There are two main types of collaborative recommendation systems:

- User-based collaborative system- Based on finding similar users and find the items those users have liked but we have not tried yet.

- Item-based collaborative system- in this case, we will find similar products to the one the user has bought and we will recommend those products that are similar to those which has ratted as best.

In addition, two systems can be combined creating hybrid models, as in the case of ensemble models in Machine Learning.

## 2. Methods and Analysis
## 2.1 Data Preparation
For this project, a movie recommendation system will be created using the MovieLens dataset. The entire latest MovieLens dataset is here. It is used the 10M version of the MovieLens dataset to make the computation a little easier.

This section is to download and prepare the dataset used in the analysis. Dataset is split in two parts; the training set called edx and the evaluation set called validation with 90% and 10% of the original dataset respectively.

The edx set is set in two parts, the train set and test set with 90% and 10% of edx set respectively. The model is built and trained in the train set and tested in the test set until the RMSE target is achieved, then finally train the model again in the entire edx set and validate in the validation set. The name of this method is cross-validation.

I will use echo=FALSE to hide the code in the output. The code is evaluated when the Rmd file is knit, however only the output is rendered on the output document.

```{r load packages, echo=FALSE, comment = ''}
# Load packages

if(!require(tidyverse)) 
install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret))
install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table))
install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
           col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# R version 4.0.4:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
 						title = as.character(title),
 						genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") 
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
      semi_join(edx, by = "movieId") %>%
      semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

The edx set is used for training and testing, and the validation set is used for final validation to simulate the new data.

Here, the edx set is split in 2 parts: the training set and the test set.

The model building is done in the training set, and the test set is used to test the model. When the model is complete, the validation set is used to calculate the final RMSE.

The same procedure is used to create edx and validation sets.

The training set will be 90% of edx data and the test set will be the remaining 10%.

```{r training set, echo=FALSE, comment = ''}
# The training set will be 90% of edx data and the test set will be the remaining 10%.
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
train_set <- edx[-test_index,]
temp <- edx[test_index,]

# Make sure userId and movieId in test set are also in train set
test_set <- temp %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

# Add rows removed from test set back into train set
removed <- anti_join(temp, test_set)
train_set <- rbind(train_set, removed)

rm(test_index, temp, removed)
```

From this initial exploration, we discover that edx has 6 columns:

userId        integer
movieId       integer
rating        numeric
timestamp     numeric
title         character
genres		    character

How many rows and columns are there in the edx dataset?

```{r dimension, echo=FALSE, comment = ''}
# To know the dimension (number of columns and rows) 
dim(edx)
```

The next table shows the structure and content of edx dataset

The dataset is in tidy format, i.e. each row has one observation and the column names are the features. The rating column is the desired outcome. The user information is stored in userId; the movie information is both in movieId and title columns. The rating date is available in timestamp measured in seconds since January 1st, 1970. Each movie is tagged with one or more genre in the genres column.

```{r head, echo=FALSE, comment = ''}
# To get the first parts of a vector, matrix, table, data frame or function it can be used head()
head(edx)
```

The next sections discover more details about each feature and outcome.

## 2.2.1	Genres
Along with the movie title, MovieLens provides the list of genres for each movie. Although this information can be used to make better predictions, this research doesn’t use it. However it’s worth exploring this information as well.
The data set contains 797 different combinations of genres. Here is the list of the first six.

```{r list genres, echo=FALSE, comment = ''}
# List of genres for each movie
edx %>% group_by(genres) %>% 
  summarise(n=n()) %>%
  head()
```

The table above shows that several movies are classified in more than one genre. The number of genres in each movie is listed in this table, sorted in descend order.

```{r number genres, echo=FALSE, comment = ''}
# The number of genres in each movie is listed in this table, sorted in descend order.
tibble(count = str_count(edx$genres, fixed("|")), genres = edx$genres) %>% 
  group_by(count, genres) %>%
  summarise(n = n()) %>%
  arrange(-count) %>% 
  head()
```

## 2.2.2	Date
The rating period was collected over almost 14 years.

```{r gather rating period, echo=FALSE, comment = ''}
# Gathering for over almost 14 years of rating period 
library(lubridate)
tibble(`Initial Date` = date(as_datetime(min(edx$timestamp), origin="1970-01-01")),
       `Final Date` = date(as_datetime(max(edx$timestamp), origin="1970-01-01"))) %>%
  mutate(Period = duration(max(edx$timestamp)-min(edx$timestamp)))

library(ggplot2)

if(!require(ggthemes)) 
  install.packages("ggthemes", repos = "http://cran.us.r-project.org")
if(!require(scales)) 
  install.packages("scales", repos = "http://cran.us.r-project.org")
edx %>% mutate(year = year(as_datetime(timestamp, origin="1970-01-01"))) %>%
  ggplot(aes(x=year)) +
    geom_histogram(color = "white") + 
    ggtitle("Rating Distribution Per Year") +
    xlab("Year") +
    ylab("Number of Ratings") +
    scale_y_continuous(labels = comma) + 
    theme_economist()
```

The following table lists the days with more ratings. Not surprisingly, the movies are blockbusters.

```{r ratings table, echo=FALSE, comment = ''}
# The next table displays the days with more ratings
edx %>% mutate(date = date(as_datetime(timestamp, origin="1970-01-01"))) %>%
  group_by(date, title) %>%
  summarise(count = n()) %>%
  arrange(-count) %>%
  head(10)
```

## 2.2.3	Ratings
Users have the option to choose a rating value from 0.5 to 5.0, totaling 10 possible values. This is unusual scale, so most movies get a rounded value rating, as shown in the chart below.

```{r number of each ratings, echo=FALSE, comment = ''}
# Count the number of each ratings
edx %>% group_by(rating) %>% summarize(n=n())
```

How many ratings are in edx?

```{r rating distribution, echo=FALSE, comment = ''}
# Rating distribution
edx %>% group_by(rating) %>% 
  summarise(count=n()) %>%
  ggplot(aes(x=rating, y=count)) + 
    geom_line() +
    geom_point() +
    scale_y_log10(breaks = trans_breaks("log10", function(x) 10^x),
                labels = trans_format("log10", math_format(10^.x))) +
    ggtitle("Rating Distribution", subtitle = "Higher ratings are prevalent.") + 
    xlab("Rating") +
    ylab("Count") +
    theme_economist()
```

## 2.2.4	Movies
There are 10,681 different movies in the edx set. Some of them are rated more than others, since many movies are watched by few users and blockbusters tend to have more ratings.

```{r distribution of movies, echo=FALSE, comment = ''}
# Distribution of movies
edx %>% group_by(movieId) %>%
  summarise(n=n()) %>%
  ggplot(aes(n)) +
    geom_histogram(color = "white") +
    scale_x_log10() + 
    ggtitle("Distribution of Movies", 
            subtitle = "The distribution is almost symetric.") +
    xlab("Number of Ratings") +
    ylab("Number of Movies") + 
    theme_economist()
```

## 2.2.5	Users
There are 71,567 users of the online movie recommender service MovieLens.
The majority of users rate few movies, while a few users rate more than a thousand movies.
5% users rated less than 20 movies.

```{r idistribution of users, echo=FALSE, comment = ''}
# Distribution of users
edx %>% group_by(userId) %>%
  summarise(n=n()) %>%
  arrange(n) %>%
  head()
```

The user distribution is right skewed.

```{r user distribution, echo=FALSE, comment = ''}
# User distribution plot
edx %>% group_by(userId) %>%
  summarise(n=n()) %>%
  ggplot(aes(n)) +
    geom_histogram(color = "white") +
    scale_x_log10() + 
    ggtitle("Distribution of Users", 
            subtitle="The distribution is right skewed.") +
    xlab("Number of Ratings") +
    ylab("Number of Users") + 
    scale_y_continuous(labels = comma) + 
    theme_economist()
```

Show the heatmap of users x movies
This user-movie matrix is sparse, with the vast majority of empty cells. Notice that four movies have more ratings, and one or two users are more active.

```{r users x movies, echo=FALSE, comment = ''}
# User x Movie Matrix
users <- sample(unique(edx$userId), 100)
edx %>% filter(userId %in% users) %>%
  select(userId, movieId, rating) %>%
  mutate(rating = 1) %>%
  spread(movieId, rating) %>% 
  select(sample(ncol(.), 100)) %>% 
  as.matrix() %>% t(.) %>%
  image(1:100, 1:100,. , xlab="Movies", ylab="Users")
abline(h=0:100+0.5, v=0:100+0.5, col = "grey")
title("User x Movie Matrix")
```

## 2.3	Data Cleaning
As previously discussed, several features can be used to predict the rating for a given user. However, many predictors increases the model complexity and requires more computer resources, so in this research the estimated rating uses only movie and user information.

```{r data cleaning, echo=FALSE, comment = ''}
# Data cleaning
train_set <- train_set %>% select(userId, movieId, rating, title)
test_set  <- test_set  %>% select(userId, movieId, rating, title)
```

## 2.4	Modeling
## 2.4.1	Random Prediction
A very simple model is just randomly predict the rating using the probability distribution observed during the data exploration. For example, if we know the probability of all users giving a movie a rating of 3 is 10%, then we may guess that 10% of the ratings will have a rating of 3.

Such prediction sets the worst error we may get, so any other model should provide better result.

## 2.4.2	Linear Model
The simplest model predicts all users will give the same rating to all movies and assumes the movie to movie variation is the randomly distributed error. Although the predicted rating can be any value, statistics theory says that the average minimizes the RMSE, so the initial prediction is just the average of all observed ratings, as described in this formula:

$\hat{Y}_{u,i} = {\mu} + {\epsilon}_{i,u}$

Where $\hat{Y}$ is the predicted rating, ${\mu}$ is the mean of observed data and ${\epsilon}_{i,u}$ is the error distribution. Any value other than the mean increases the RMSE, so this is a good initial estimation.

Part of the movie to movie variability can be explained by the fact that different movies have different rating distribution. This is easy to understand, since some movies are more popular than others and the public preference varies. This is called movie effect or movie bias, and is expressed as ${b_i}$ in this formula:

$\hat{Y}_{u,i} = {\mu} +  b_{i} + {\epsilon}_{i,u}$

The movie effect can be calculated as the mean of the difference between the observed rating ${y}$ and the mean ${\mu}$.

$\hat{b}_{i} = \frac{1}{N} \sum_{i=1}^{N} (y_{i} - \hat{\mu})$

Similar to the movie effect, different users have different rating pattern or distribution. For example, some users like most movies and consistently rate 4 or 5, while other users dislike most movies rating 1 or 2. This is called user effect or user bias and is expressed in this formula:

$\hat{b}_{u} = \frac{1}{N} \sum_{i=1}^{N} (y_{u,i} - \hat{b}_{i} - \hat{\mu})$

The prediction model that includes the user effect becomes:

$\hat{Y}_{u,i} = {\mu} +  b_{i} + +  b_{u} + {\epsilon}_{u,i}$

Movies can be grouped into categories or genres, with different distributions. In general, movies in the same genre get similar ratings. In this project we won’t evaluate the genre effect.

## 2.4.3	Regularization
The linear model provides a good estimation for the ratings, but doesn’t consider that many movies have very few number of ratings, and some users rate very few movies. This means that the sample size is very small for these movies and these users. Statistically, this leads to large estimated error.

The estimated value can be improved adding a factor that penalizes small sample sizes and have have little or no impact otherwise. Thus, estimated movie and user effects can be calculated with these formulas:

$\hat{b}_{i} = \frac{1}{{n}_{i} + {\lambda}} \sum_{u=1}^{{n}_{i}} (y_{u,i} - \hat{\mu})$

$\hat{b}_{u} = \frac{1}{{n}_{u} + {\lambda}} \sum_{i=1}^{{n}_{u}} (y_{u,i} - \hat{b}_{i} - \hat{\mu})$

For values of ${N}$ smaller than or similar to $\lambda$, $\hat{b}_{i}$ and $\hat{b}_{u}$ is smaller than the original values, whereas for values of ${N}$ much larger than $\lambda$, $\hat{b}_{i}$ and $\hat{b}_{u}$ change very little.

An effective method to choose $\lambda$ that minimizes the RMSE is running simulations with several values of $\lambda$.

## 2.4.4	Matrix Factorization
Matrix factorization is widely used machine learning tool for predicting ratings in recommendation systems. This method became widely known during the Netflix Prize challenge.

The data can be converted into a matrix such that each user is in a row, each movie is in a column and the rating is in the cell, then the algorithm attempts to fill in the missing values. The table below provides a simple example of a 4×54×5 matrix.

The concept is to approximate a large rating matrix ${R}_{mxn}$ into the product of two lower dimension matrices ${P}_{kxm}$ and ${Q}_{kxn}$, such that

${R}\approx {P}Q$

The ${R}$ recosystem11 package provides methods to decompose the rating matrix and estimate the user rating, using parallel matrix factorization.

## 3.	Results
This section presents the code and results of the models.

## 3.1	Model Evaluation Functions

Definition of the loss functions.

```{r MAE, echo=FALSE, comment = ''}
# Define Mean Absolute Error (MAE)
MAE <- function(true_ratings, predicted_ratings){
  mean(abs(true_ratings - predicted_ratings))
}
```

```{r MSE, echo=FALSE, comment = ''}
# Define Mean Squared Error (MSE)
MSE <- function(true_ratings, predicted_ratings){
  mean((true_ratings - predicted_ratings)^2)
}
```

```{r RMSE, echo=FALSE, comment = ''}
# Define Root Mean Squared Error (RMSE)
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

## 3.2	Random Prediction
The first model randomly predicts the ratings using the observed probabilities in the training set. First, the probability is calculated of each rating in the training set, then it is predicted the rating for the test set and compare with actual rating. Any model should be better than this one.

Since the training set is a sample of the entire population and the real distribution of ratings is not known, the Monte Carlo simulation with replacement provides a good approximation of the rating distribution.

A Monte Carlo simulation is a model used to predict the probability of different outcomes when the intervention of random variables is present. Monte Carlo simulations help to explain the impact of risk and uncertainty in prediction and forecasting models.

```{r probability, echo=FALSE, comment = ''}
set.seed(4321, sample.kind = "Rounding")

# Create the probability of each rating
p <- function(x, y) mean(y == x)
rating <- seq(0.5,5,0.5)

# Estimate the probability of each rating with Monte Carlo simulation
B <- 10^3
M <- replicate(B, {
  s <- sample(train_set$rating, 100, replace = TRUE)
  sapply(rating, p, y= s)
})
prob <- sapply(1:nrow(M), function(x) mean(M[x,]))

# Predict random ratings
y_hat_random <- sample(rating, size = nrow(test_set), 
                       replace = TRUE, prob = prob)

# Create a table with the error results
result <- tibble(Method = "Project Goal", RMSE = 0.8649, MSE = NA, MAE = NA)
result <- bind_rows(result, 
                    tibble(Method = "Random prediction", 
                           RMSE = RMSE(test_set$rating, y_hat_random),
                           MSE  = MSE(test_set$rating, y_hat_random),
                           MAE  = MAE(test_set$rating, y_hat_random)))
```

The RMSE of random prediction is very high.

Root Mean Square Error (RMSE) is a standard way to measure the error of a model in predicting quantitative data.

Heuristically RMSE can be thought of as some kind of (normalized) distance between the vector of predicted values and the vector of observed values.

In data science, RMSE has a double purpose:
- To serve as a heuristic for training models
- To evaluate trained models for usefulness / accuracy

```{r result, echo=FALSE, comment = ''}
# RMSE of random prediction
result
```

## 3.3	Linear Model
Linear regression is used to predict the value of a continuous variable Y based on one or more input predictor variables X. The aim is to establish a mathematical formula between the the response variable (Y) and the predictor variables (Xs).

Linear model is built based on the formula:

$\hat{y} = {\mu} +  b_{i} + +  b_{u} + {\epsilon}_{u,i}$

## 3.3.1	Initial Prediction
The initial prediction is just the mean of the ratings, ${\mu}$

$\hat{y} = {\mu} + {\epsilon}_{u,i}$

```{r mu, echo=FALSE, comment = ''}
# Mean of observed values
mu <- mean(train_set$rating)

# Update the error table  
result <- bind_rows(result, 
                    tibble(Method = "Mean", 
                           RMSE = RMSE(test_set$rating, mu),
                           MSE  = MSE(test_set$rating, mu),
                           MAE  = MAE(test_set$rating, mu)))

# Show the RMSE improvement  
result
```

## 3.3.2	Include Movie Effect (bi)

${b}_{i}$ is the movie effect (bias) for movie ${i}$.

$\hat{y} = {\mu} +  b_{i} + {\epsilon}_{u,i}$

```{r bi, echo=FALSE, comment = ''}
# Movie effects (bi)
bi <- train_set %>% 
group_by(movieId) %>% 
summarize(b_i = mean(rating - mu))
head(bi)
```

The movie effect is normally left skewed distributed.

```{r bi dist, echo=FALSE, comment = ''}
# Movie effect distribution
bi %>% ggplot(aes(x = b_i)) + 
  geom_histogram(bins=10, col = I("black")) +
  ggtitle("Movie Effect Distribution") +
  xlab("Movie effect") +
  ylab("Count") +
  scale_y_continuous(labels = comma) + 
  theme_economist()
```

```{r y_hat_bi, echo=FALSE, comment = ''}
# Predict the rating with mean + bi  
y_hat_bi <- mu + test_set %>% 
  left_join(bi, by = "movieId") %>% 
  .$b_i

# Calculate the RMSE  
result <- bind_rows(result, 
                    tibble(Method = "Mean + bi", 
                           RMSE = RMSE(test_set$rating, y_hat_bi),
                           MSE  = MSE(test_set$rating, y_hat_bi),
                           MAE  = MAE(test_set$rating, y_hat_bi)))

# Show the RMSE improvement  
result
```

## 3.3.3	Include User Effect (bu)

${b}_{u}$ is the user effect (bias) for user ${u}$.

$\hat{y}_{u,i} = {\mu} +  b_{i} + +  b_{u} + {\epsilon}_{u,i}$

Predict the rating with ${\mu} +  b_{i} + +  b_{u}$

```{r bu, echo=FALSE, comment = ''}
# User effect (bu)
bu <- train_set %>% 
  left_join(bi, by = 'movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

# Prediction
y_hat_bi_bu <- test_set %>% 
  left_join(bi, by='movieId') %>%
  left_join(bu, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred

# Update the results table
result <- bind_rows(result, 
                    tibble(Method = "Mean + bi + bu", 
                           RMSE = RMSE(test_set$rating, y_hat_bi_bu),
                           MSE  = MSE(test_set$rating, y_hat_bi_bu),
                           MAE  = MAE(test_set$rating, y_hat_bi_bu)))

# Show the RMSE improvement  
result
```

The user effect is normally distributed.

```{r User effect dist, echo=FALSE, comment = ''}
# User effect distribution
train_set %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_u)) + 
    geom_histogram(color = "black") + 
    ggtitle("User Effect Distribution") +
    xlab("User Bias") +
    ylab("Count") +
    scale_y_continuous(labels = comma) + 
    theme_economist()
```

## 3.3.4	Evaluating the model result

The RMSE improved from the initial estimation based on the mean. However, it is necessary to check if the model makes good ratings predictions.

Check the 10 largest residual differences

```{r evaluating model result, echo=FALSE, comment = ''}
# Evaluating the model result
train_set %>% 
  left_join(bi, by='movieId') %>%
  mutate(residual = rating - (mu + b_i)) %>%
  arrange(desc(abs(residual))) %>%  
  slice(1:10)

# Movie titles
titles <- train_set %>% 
  select(movieId, title) %>% 
  distinct()

# Top 10 best movies
# Unknown movies
bi %>% 
  inner_join(titles, by = "movieId") %>% 
  arrange(-b_i) %>% 
  select(title) %>%
  head()

# Top 10 worst movies
# Unknown movies
bi %>% 
  inner_join(titles, by = "movieId") %>% 
  arrange(b_i) %>% 
  select(title) %>%
  head()

# Number of ratings for 10 best movies
train_set %>% 
  left_join(bi, by = "movieId") %>%
  arrange(desc(b_i)) %>% 
  group_by(title) %>% 
  summarise(n = n()) %>% 
  slice(1:10)

# Check if the model makes good ratings predictions
train_set %>% count(movieId) %>% 
  left_join(bi, by="movieId") %>% 
  arrange(desc(b_i)) %>% 
  slice(1:10) %>% 
  pull(n)
```

## 3.4	Regularization

Now, regularize the user and movie effects adding a penalty factor ${\lambda}$, which is a tuning parameter. defined a number of values for  and use the regularization function to pick the best value that minimizes the RMSE.

Regularization permits us to penalize large estimates that are formed using small sample sizes. It has commonalities with the Bayesian approach that shrunk predictions.

```{r regularization, echo=FALSE, comment = ''}
# Regularization
regularization <- function(lambda, trainset, testset){

# Mean
  mu <- mean(trainset$rating)

# Movie effect (bi)
  b_i <- trainset %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+lambda))

# User effect (bu)  
  b_u <- trainset %>% 
    left_join(b_i, by="movieId") %>%
    filter(!is.na(b_i)) %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))

# Prediction: mu + bi + bu  
  predicted_ratings <- testset %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    filter(!is.na(b_i), !is.na(b_u)) %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, testset$rating))
}

# Define a set of lambdas to tune
lambdas <- seq(0, 10, 0.25)

# Tune lambda
rmses <- sapply(lambdas, 
                regularization, 
                trainset = train_set, 
                testset = test_set)

# Plot the lambda vs RMSE
tibble(Lambda = lambdas, RMSE = rmses) %>%
  ggplot(aes(x = Lambda, y = RMSE)) +
    geom_point() +
    ggtitle("Regularization", 
            subtitle = "Pick the penalization that gives the lowest RMSE.") +
    theme_economist()
```

Next, I apply the best to the linear model.

```{r lambda, echo=FALSE, comment = ''}
# We pick the lambda that returns the lowest RMSE.
lambda <- lambdas[which.min(rmses)]

# Then, we calculate the predicted rating using the best parameters achieved from regularization.  
mu <- mean(train_set$rating)

# Movie effect (bi)
b_i <- train_set %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+lambda))

# User effect (bu)
b_u <- train_set %>% 
  left_join(b_i, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))

# Prediction
y_hat_reg <- test_set %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)

# Update the result table
result <- bind_rows(result, 
                    tibble(Method = "Regularized bi and bu", 
                           RMSE = RMSE(test_set$rating, y_hat_reg),
                           MSE  = MSE(test_set$rating, y_hat_reg),
                           MAE  = MAE(test_set$rating, y_hat_reg)))

# Regularization made a small improvement in RMSE
result
```

## 3.5	Matrix Factorization

Matrix factorization approximates a large user-movie matrix into the product of two smaller dimension matrices. Information in the train set is stored in tidy format, with one observation per row, so it needs to be converted to the user-movie matrix before using matrix factorization. This code executes this transformation.

```{r matrix factorization, eval = FALSE, comment = ''}
# Matrix factorization
train_data <- train_set %>% 
  select(userId, movieId, rating) %>% 
  spread(movieId, rating) %>%
  as.matrix()
```

The code above uses more memory than a commodity laptop is able to process, so I use an alternative method: the recosystem package, which provides the complete solution for a recommendation system using matrix factorization. 

When using *eval = FALSE*, I do not evaluate (or run) this code chunk when knitting the RMD document. The code in this chunk will still render in my knitted output, however it will not be evaluated or run by R
.
The package vignette describes how to use recosystem:

Usage of recosystem

The usage of recosystem is quite simple, mainly consisting of the following steps:

1.	Create a model object (a Reference Class object in R) by calling Reco().
2.	(Optionally) call the $tune() method to select best tuning parameters along a set of candidate values.
3.	Train the model by calling the train() method. A number of parameters can be set inside the function, possibly coming from the result of tune().
4.	(Optionally) export the model via output(), i.e. write the factorization matrices ${P}$ and ${Q}$ into files or return them as ${R}$ objects.
5.	Use the $predict() method to compute predicted values.

```{r recosystem, echo=FALSE, comment = ''}
# recosystem
if(!require(recosystem)) 
  install.packages("recosystem", repos = "http://cran.us.r-project.org")
set.seed(123, sample.kind = "Rounding") # This is a randomized algorithm

# Convert the train and test sets into recosystem input format
train_data <-  with(train_set, data_memory(user_index = userId, 
                                           item_index = movieId, 
                                           rating     = rating))
test_data  <-  with(test_set,  data_memory(user_index = userId, 
                                           item_index = movieId, 
                                           rating     = rating))

# Create the model object
r <-  recosystem::Reco()

# Select the best tuning parameters
opts <- r$tune(train_data, opts = list(dim = c(10, 20, 30), 
                                       lrate = c(0.1, 0.2),
                                       costp_l2 = c(0.01, 0.1), 
                                       costq_l2 = c(0.01, 0.1),
                                       nthread  = 4, niter = 10))

# Train the algorithm  
r$train(train_data, opts = c(opts$min, nthread = 4, niter = 20))

# Calculate the predicted values  
y_hat_reco <-  r$predict(test_data, out_memory())
head(y_hat_reco, 10)

# Matrix factorization improved substantially the RMSE.
result <- bind_rows(result, 
                    tibble(Method = "Matrix Factorization - recosystem", 
                           RMSE = RMSE(test_set$rating, y_hat_reco),
                           MSE  = MSE(test_set$rating, y_hat_reco),
                           MAE  = MAE(test_set$rating, y_hat_reco)))
result
```

## 3.6	Final Validation

As seen from the result table, regularization and matrix factorization achieved the target RMSE. So, finally train the complete edx set with both models and calculate the RMSE in the validation set. The project goal is achieved if the RMSE stays below the target.

## 3.6.1	Linear Model with Regularization

During the training and testing phases, the linear model with regularization achieved the target RMSE with a small margin. Here I do the final validation with the validation set.

```{r linear model with regularization, echo=FALSE, comment = ''}
# Linear model with regularization 
mu_edx <- mean(edx$rating)

# Movie effect (bi)
b_i_edx <- edx %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu_edx)/(n()+lambda))

# User effect (bu)
b_u_edx <- edx %>% 
  left_join(b_i_edx, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu_edx)/(n()+lambda))

# Prediction
y_hat_edx <- validation %>% 
  left_join(b_i_edx, by = "movieId") %>%
  left_join(b_u_edx, by = "userId") %>%
  mutate(pred = mu_edx + b_i + b_u) %>%
  pull(pred)

# Update the results table
result <- bind_rows(result, 
                    tibble(Method = "Final Regularization (edx vs validation)", 
                           RMSE = RMSE(validation$rating, y_hat_edx),
                           MSE  = MSE(validation$rating, y_hat_edx),
                           MAE  = MAE(validation$rating, y_hat_edx)))

# Show the RMSE improvement
result
```

As expected, the RMSE calculated on the validation set (0.8648) is lower than the target of 0.8649 and slightly higher than the RMSE of the test set (0.8641).

```{r 10 best movies, echo=FALSE, comment = ''}
# Top 10 best movies
# Validation
validation %>% 
  left_join(b_i_edx, by = "movieId") %>%
  left_join(b_u_edx, by = "userId") %>% 
  mutate(pred = mu_edx + b_i + b_u) %>% 
  arrange(-pred) %>% 
  group_by(title) %>% 
  select(title) %>%
  head(10)

# Top 10 worst movies
validation %>% 
  left_join(b_i_edx, by = "movieId") %>%
  left_join(b_u_edx, by = "userId") %>% 
  mutate(pred = mu_edx + b_i + b_u) %>% 
  arrange(pred) %>% 
  group_by(title) %>% 
  select(title) %>%
  head(10)
```

## 3.6.2	Matrix Factorization

The initial test shows that matrix factorization gives the best RMSE. Now it’s time to validate with the entire edx and validation sets.

```{r matrix fact, echo=FALSE, comment = ''}
# Matrix factorization
set.seed(1234, sample.kind = "Rounding")

# Convert 'edx' and 'validation' sets to recosystem input format
edx_reco <-  with(edx, data_memory(user_index = userId, 
                                   item_index = movieId, 
                                   rating = rating))

validation_reco  <-  with(validation, data_memory(user_index = userId, 
                                                  item_index = movieId, 
                                                  rating = rating))

# Create the model object
r <-  recosystem::Reco()

# Tune the parameters
opts <-  r$tune(edx_reco, opts = list(dim = c(10, 20, 30), 
                                     lrate = c(0.1, 0.2),
                                     costp_l2 = c(0.01, 0.1), 
                                     costq_l2 = c(0.01, 0.1),
                                     nthread  = 4, niter = 10))

# Train the model
r$train(edx_reco, opts = c(opts$min, nthread = 4, niter = 20))

# Calculate the prediction
y_hat_final_reco <-  r$predict(validation_reco, out_memory())

# Update the result table
result <- bind_rows(result, 
                    tibble(Method = "Final Matrix Factorization - recosystem", 
                           RMSE = RMSE(validation$rating, y_hat_final_reco),
                           MSE  = MSE(validation$rating, y_hat_final_reco),
                           MAE  = MAE(validation$rating, y_hat_final_reco)))
```

The final RMSE with matrix factorization is 0.7826, 9.5% better than the linear model with regularization (0.8648).

```{r RMSE improvement, echo=FALSE, comment = ''}
# Show the RMSE improvement
result
```

Now, let’s check the best and worst movies predicted with matrix factorization.

```{r Top 10 best movies, echo=FALSE, comment = ''}
#Top 10 best movies:
# Top 10 best movies with matrix factorization
tibble(title = validation$title, rating = y_hat_final_reco) %>%
  arrange(-rating) %>% 
  group_by(title) %>% 
  select(title) %>%
  head(10)
```

```{r Top 10 worst movies, echo=FALSE, comment = ''}
# Top 10 worst movies with matrix factorization
tibble(title = validation$title, rating = y_hat_final_reco) %>%
  arrange(rating) %>% 
  group_by(title) %>% 
  select(title) %>%
  head(10)
```

## 4. Conclusion

The initial challenge was to collect and prepare the dataset for analysis, later the necessity to explore the information looking for insights that could help during the model building.

Next, the creation of a random model that predicts the rating based on the probability distribution of each rating. This model gives the worst result.

The creation of a linear model with a very simple model that is the mean of the observed ratings. Continuing, a movie and user effects were added, that models the user behavior and movie distribution. With regularization a penalty value was added for the movies and users with few number of ratings. The linear model achieved the RMSE of 0.8648, successfully passing the target of 0.8649.

Finally, the recosystem package was evaluated, that implements the LIBMF algorithm, and achieved the RMSE of 0.7826.

## 4.1 Limitations

Some machine learning algorithms are computationally expensive to run in a commodity laptop and therefore were unable to test. The required amount of memory far exceeded the available in a commodity laptop, even with increased virtual memory.

Only two predictors are used, the movie and user information, not considering other features. Modern recommendation system models use many predictors, such as genres, bookmarks, playlists, etc.

The model works only for existing users, movies and rating values, so the algorithm must run every time a new user or movie is included, or when the rating changes. This is not an issue for small client base and a few movies, but may become a concern for large data sets. The model should consider these changes and update the predictions as information changes.

There is no initial recommendation for a new user or for users that usually don’t rate movies. Algorithms that uses several features as predictors can overcome this issue.

## 4.2 Future Work

This report briefly describes simple models that predicts ratings. There are two other widely adopted approaches not discussed here: content-based and collaborative filtering. The recommenderlab package implements these methods and provides an environment to build and test recommendation systems.

Besides recommenderlab, there are other packages for building recommendation systems available in The Comprehensive R Archive Network (CRAN) website.

## References

[1.	Rafael A. Irizarry (2019). Introduction to Data Science](https://www.amazon.com/Introduction-Data-Science-Prediction-Algorithms/dp/0367357984?language=en_US)

[2. Ander Fernandez Jauregui (2021). How to Code a recommendation System in R](https://anderfernandez.com/en/blog/how-to-code-a-recommendation-system-in-r/)

[3. Leah Wasser, NEON Data Skills. How to use R Markdown Code Chunks](https://www.earthdatascience.org/courses/earth-analytics/document-your-science/rmarkdown-code-chunks-comments-knitr/)

[4. R Markdown Syntax: Hyperlinks, Images & Tables](https://ucsbcarpentry.github.io/R-markdown/04-links-images-tables/index.html)

[5. Yihui Xie (2005-2020).Chunk options and package options](https://yihui.org/knitr/options/#chunk_options)

